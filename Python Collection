import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib
import matplotlib.pyplot as plt

#The error metric: RMSE on the log of thte sale prices.
from sklearn.metrics import mean_squared_error
def rmse(y_true,y_pred):
    return np.sqrt(mean_squared_error(y_true,y_pred))
    

#Load data:
train_df = pd.read_csv("../input/train.csv")
test_df = pd.read_csv("../input/test.csv")

#Show DataFrame type of data in a histogram chart
matplotlib.rcParams['figure.figsize'] = (12.0, 6.0)
prices = pd.DataFrame({"price":train_df["SalePrice"], "log(price + 1)":np.log1p(train_df["SalePrice"])})
prices.hist()

#Get unique value from a list
a = list(set(list(train_df["HalfBath"])))
print (a)

#Fill value into field
# The test example 1116 only has GarageType but no other information. We'll 
# assume it does not have a garage.
test_df.loc[1116, "GarageType"] = np.nan

# Used to convert categorical features into ordinal numbers.
# (There's probably an easier way to do this, but it works.)
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

def factorize(df, factor_df, column, fill_na=None):
    factor_df[column] = df[column]
    if fill_na is not None:
        factor_df[column].fillna(fill_na, inplace=True)
    le.fit(factor_df[column].unique())
    factor_df[column] = le.transform(factor_df[column])
    return factor_df

# 定义若干个对数据进行清理的函数，这些函数主要作用在pandas的DataFrame数据类型上
# 查看数据集属性值得确实情况
def show_missing(houseprice): #houseprice should be a DataFrame structure parameter
    missing = houseprice.columns[houseprice.isnull().any()].tolist()
    return missing

# 查看 categorical 特征的值情况, category distribution
def cat_exploration(houseprice, column):
    print (houseprice[column].value_counts())
    
# 对数据集中某一列的缺失值进行tidia
def cat_imputation(houseprice, column, value):
    houseprice.loc[houseprice[column].isnull,column] =value



#Check correlation between two parameter: test_data['LotFrontage'] vs. test_data['LotArea']
print(test_data['LotFrontage'].corr(test_data['LotArea'])) 

#SQL- Where Clause
#Following statement print full test_data when MSZoning column is null.
print(test_data[test_data['MSZoning'].isnull()==True])
#Following statemnet equals to:
# "Select basement_cols defined columns 
#  from train_data where train_data.BsmtQual is null"
print(train_data[basement_cols][train_data['BsmtQual'].isnull()==True])

#Replace a value with Where Clause
#Update RPC column
new_calllog.loc[new_calllog['agent_system_term_code'].isin(['PP','PTP']),['rpc']]='Yes'
loc[<condition>, <update colume name>] = <update value>

#pd.crosstab: is to pivot table and count occurancies
print(pd.crosstab(test_data.BsmtCond,test_data.BsmtQual))


## In this "For in " statement, test_data.columns is a list of all column names from test_data set.
for col in test_data.columns:
    t1 = test_data[col].dtype
    t2 = train_data[col].dtype
    if t1!=t2:
        print(col,t1,t2)
        
 #SQL - Merge, Join and Concate 
https://pandas.pydata.org/pandas-docs/stable/merging.html


#Change column name in DataFrame:
df_test=df_test.rename(index = str, columns= {"NoteUser0":"NoteUser"})


#Case When in DataFrame Fields: 
ch['IfAvgHigher']='Unknown'
ch.loc[ch['AvgCharge'] >= ch['MonthlyCharges'], 'IfAvgHigher'] = 'Yes'
ch.loc[ch['AvgCharge'] < ch['MonthlyCharges'], 'IfAvgHigher'] = 'No'

#Check if dataframe column contains certain value:
df[df['A'].str.contains("hello")]
